"""
Adventure Works Analytics Pipeline - Enterprise Edition
Usando Astro CLI + dbt + Databricks

Autor: Diego Brito
Data: 2025-01-15
VersÃ£o: 2.0 - Astro Enterprise
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email_operator import EmailOperator
from airflow.operators.dummy_operator import DummyOperator
import logging

# ConfiguraÃ§Ãµes otimizadas para Astro
default_args = {
    'owner': 'diego.brito@indicium.tech',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email': ['diego.brito@indicium.tech'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False,
    'max_active_runs': 1,
    'max_active_tasks': 16,
}

# DAG com configuraÃ§Ãµes empresariais
dag = DAG(
    'adventure_works_analytics_enterprise',
    default_args=default_args,
    description='Pipeline Analytics Adventure Works - Enterprise com Astro + dbt + Databricks',
    schedule_interval='0 6 * * *',  # DiÃ¡rio Ã s 6h
    catchup=False,
    doc_md=__doc__,
    tags=['adventure-works', 'dbt', 'databricks', 'enterprise', 'astro'],
)

# FunÃ§Ã£o para logging avanÃ§ado
def log_pipeline_start(**context):
    logging.info(f"ðŸš€ Iniciando Pipeline Adventure Works Enterprise - Run: {context['run_id']}")
    logging.info(f"ðŸ“… Data de execuÃ§Ã£o: {context['ds']}")
    logging.info(f"ðŸ—ï¸ Ambiente: Astro CLI + dbt + Databricks")
    logging.info(f"ðŸ‘¤ Executado por: {context['dag'].owner}")
    return "Pipeline iniciado com sucesso"

def log_pipeline_end(**context):
    logging.info(f"ðŸŽ‰ Pipeline Adventure Works Enterprise concluÃ­do com sucesso!")
    logging.info(f"ðŸ“… Data de execuÃ§Ã£o: {context['ds']}")
    logging.info(f"ðŸ—ï¸ Ambiente: Astro CLI + dbt + Databricks")
    logging.info(f"âœ… Status: Pipeline executado com sucesso")
    return "Pipeline concluÃ­do"

# ConfiguraÃ§Ãµes de diretÃ³rio dbt
DBT_PROJECT_DIR = '/opt/airflow/dbt_project'
DBT_PROFILES_DIR = '/opt/airflow/dbt_project'

# Tasks principais
iniciar_pipeline = PythonOperator(
    task_id='iniciar_pipeline_enterprise',
    python_callable=log_pipeline_start,
    dag=dag,
)

verificar_ambiente = BashOperator(
    task_id='verificar_ambiente_dbt',
    bash_command=f'''
    echo "ðŸ” Verificando ambiente dbt..." && \\
    dbt --version && \\
    echo "âœ… Ambiente dbt verificado"
    ''',
    dag=dag,
)

executar_staging = BashOperator(
    task_id='executar_modelos_staging',
    bash_command=f'''
    echo "ðŸ”„ Executando modelos staging..." && \\
    echo "âœ… Modelos staging executados (simulado)"
    ''',
    dag=dag,
)

executar_intermediate = BashOperator(
    task_id='executar_modelos_intermediate',
    bash_command=f'''
    echo "ðŸ”„ Executando modelos intermediate..." && \\
    echo "âœ… Modelos intermediate executados (simulado)"
    ''',
    dag=dag,
)

executar_marts = BashOperator(
    task_id='executar_modelos_marts',
    bash_command=f'''
    echo "ðŸ”„ Executando modelos marts..." && \\
    echo "âœ… Modelos marts executados (simulado)"
    ''',
    dag=dag,
)

executar_testes = BashOperator(
    task_id='executar_testes_dbt',
    bash_command=f'''
    echo "ðŸ§ª Executando testes de qualidade..." && \\
    echo "âœ… Testes de qualidade executados (simulado)"
    ''',
    dag=dag,
)

gerar_docs = BashOperator(
    task_id='gerar_documentacao_dbt',
    bash_command=f'''
    echo "ðŸ“š Gerando documentaÃ§Ã£o..." && \\
    echo "âœ… DocumentaÃ§Ã£o gerada (simulado)"
    ''',
    dag=dag,
)

finalizar_pipeline = PythonOperator(
    task_id='finalizar_pipeline_enterprise',
    python_callable=log_pipeline_end,
    dag=dag,
)

# NotificaÃ§Ã£o de sucesso
notificacao_sucesso = DummyOperator(
    task_id='notificacao_sucesso_enterprise',
    dag=dag,
)

# Definir dependÃªncias
iniciar_pipeline >> verificar_ambiente >> executar_staging >> executar_intermediate >> executar_marts >> executar_testes >> gerar_docs >> finalizar_pipeline >> notificacao_sucesso
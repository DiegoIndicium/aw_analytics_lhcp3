"""
Adventure Works Analytics Pipeline - Enterprise Edition
Usando Astro CLI + dbt + Databricks

Autor: Diego Brito
Data: 2025-01-15
VersÃ£o: 2.0 - Astro Enterprise
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import PythonOperator
from airflow.operators.email_operator import EmailOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.utils.task_group import TaskGroup
from airflow.models import Variable
import logging
import os

# ConfiguraÃ§Ãµes otimizadas para Astro
default_args = {
    'owner': 'diego.brito@indicium.tech',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email': ['diego.brito@indicium.tech'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'catchup': False,
    'max_active_runs': 1,
    'max_active_tasks': 16,
}

# DAG com configuraÃ§Ãµes empresariais
dag = DAG(
    'adventure_works_analytics_enterprise',
    default_args=default_args,
    description='Pipeline Analytics Adventure Works - Enterprise com Astro + dbt + Databricks',
    schedule_interval='0 6 * * *',  # DiÃ¡rio Ã s 6h
    catchup=False,
    doc_md=__doc__,
    tags=['adventure-works', 'dbt', 'databricks', 'enterprise', 'astro'],
)

# FunÃ§Ã£o para logging avanÃ§ado
def log_pipeline_start(**context):
    logging.info(f"ğŸš€ Iniciando Pipeline Adventure Works Enterprise - Run: {context['run_id']}")
    logging.info(f"ğŸ“… Data de execuÃ§Ã£o: {context['ds']}")
    logging.info(f"ğŸ—ï¸ Ambiente: Astro CLI + dbt + Databricks")
    logging.info(f"ğŸ‘¤ Executado por: {context['dag'].owner}")
    return "Pipeline iniciado com sucesso"

def log_pipeline_end(**context):
    logging.info(f"ğŸ‰ Pipeline Adventure Works Enterprise concluÃ­do com sucesso!")
    logging.info(f"â±ï¸ DuraÃ§Ã£o total: {context['dag_run'].end_date - context['dag_run'].start_date}")
    return "Pipeline concluÃ­do"

# ConfiguraÃ§Ãµes de diretÃ³rio dbt
DBT_PROJECT_DIR = '/usr/local/airflow/include/dbt_project'
DBT_PROFILES_DIR = '/usr/local/airflow/include/dbt_project/profiles'

# Task Groups para organizaÃ§Ã£o empresarial
with TaskGroup("setup_pipeline", dag=dag) as setup_group:
    iniciar_pipeline = PythonOperator(
        task_id='iniciar_pipeline_enterprise',
        python_callable=log_pipeline_start,
        doc_md="InicializaÃ§Ã£o do pipeline com logging avanÃ§ado",
    )
    
    verificar_ambiente = BashOperator(
        task_id='verificar_ambiente_dbt',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ” Verificando ambiente dbt..." && \\
        dbt --version && \\
        echo "ğŸ”— Testando conexÃ£o com Databricks..." && \\
        dbt debug && \\
        echo "âœ… Ambiente dbt verificado e conectado"
        ''',
        doc_md="VerificaÃ§Ã£o do ambiente dbt e conectividade com Databricks",
    )

with TaskGroup("dbt_setup", dag=dag) as dbt_setup_group:
    limpar_ambiente_dbt = BashOperator(
        task_id='limpar_ambiente_dbt',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ§¹ Limpando ambiente dbt..." && \\
        dbt clean && \\
        echo "âœ… Ambiente limpo"
        ''',
        doc_md="Limpeza do ambiente dbt (target, logs, etc.)",
    )
    
    instalar_dependencias_dbt = BashOperator(
        task_id='instalar_dependencias_dbt',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ“¦ Instalando dependÃªncias dbt..." && \\
        dbt deps && \\
        echo "âœ… DependÃªncias instaladas com sucesso"
        ''',
        doc_md="InstalaÃ§Ã£o de dependÃªncias dbt (packages.yml)",
    )

with TaskGroup("dbt_staging", dag=dag) as staging_group:
    executar_modelos_staging = BashOperator(
        task_id='executar_modelos_staging',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ”„ Executando modelos staging..." && \\
        dbt run --select staging --full-refresh && \\
        echo "âœ… Modelos staging executados com sucesso"
        ''',
        doc_md="ExecuÃ§Ã£o de todos os modelos da camada staging",
    )

with TaskGroup("dbt_intermediate", dag=dag) as intermediate_group:
    executar_modelos_intermediate = BashOperator(
        task_id='executar_modelos_intermediate',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ”„ Executando modelos intermediate..." && \\
        dbt run --select intermediate && \\
        echo "âœ… Modelos intermediate executados com sucesso"
        ''',
        doc_md="ExecuÃ§Ã£o de todos os modelos da camada intermediate",
    )

with TaskGroup("dbt_marts", dag=dag) as marts_group:
    executar_modelos_marts = BashOperator(
        task_id='executar_modelos_marts',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ”„ Executando modelos marts..." && \\
        dbt run --select marts && \\
        echo "âœ… Modelos marts executados com sucesso"
        ''',
        doc_md="ExecuÃ§Ã£o de todos os modelos da camada marts (dimensÃµes e fatos)",
    )

with TaskGroup("quality_assurance", dag=dag) as qa_group:
    executar_testes_dbt = BashOperator(
        task_id='executar_testes_dbt',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ§ª Executando testes de qualidade..." && \\
        dbt test --store-failures && \\
        echo "âœ… Testes de qualidade executados com sucesso"
        ''',
        doc_md="ExecuÃ§Ã£o de todos os testes de qualidade de dados",
    )
    
    gerar_documentacao_dbt = BashOperator(
        task_id='gerar_documentacao_dbt',
        bash_command=f'''
        cd {DBT_PROJECT_DIR} && \\
        export DBT_PROFILES_DIR={DBT_PROFILES_DIR} && \\
        echo "ğŸ“š Gerando documentaÃ§Ã£o..." && \\
        dbt docs generate && \\
        echo "âœ… DocumentaÃ§Ã£o gerada com sucesso"
        ''',
        doc_md="GeraÃ§Ã£o da documentaÃ§Ã£o dbt (HTML)",
    )

# FinalizaÃ§Ã£o do pipeline
finalizar_pipeline = PythonOperator(
    task_id='finalizar_pipeline_enterprise',
    python_callable=log_pipeline_end,
    doc_md="FinalizaÃ§Ã£o do pipeline com logging de mÃ©tricas",
    dag=dag,
)

# NotificaÃ§Ã£o de sucesso
notificacao_sucesso = EmailOperator(
    task_id='notificacao_sucesso_enterprise',
    to=['diego.brito@indicium.tech'],
    subject='âœ… Pipeline Adventure Works Enterprise - Executado com Sucesso',
    html_content='''
    <h2>ğŸ‰ Pipeline Adventure Works Analytics Enterprise</h2>
    <p><strong>Status:</strong> âœ… Executado com Sucesso</p>
    <p><strong>Data:</strong> {{ ds }}</p>
    <p><strong>Ambiente:</strong> Astro CLI + dbt + Databricks</p>
    <p><strong>DuraÃ§Ã£o:</strong> {{ dag_run.end_date - dag_run.start_date }}</p>
    
    <h3>ğŸ“Š Resumo da ExecuÃ§Ã£o:</h3>
    <ul>
        <li>âœ… Setup e verificaÃ§Ã£o do ambiente</li>
        <li>âœ… Staging models executados</li>
        <li>âœ… Intermediate models executados</li>
        <li>âœ… Marts models executados</li>
        <li>âœ… Testes de qualidade executados</li>
        <li>âœ… DocumentaÃ§Ã£o gerada</li>
    </ul>
    
    <h3>ğŸ—ï¸ Arquitetura:</h3>
    <p>ğŸ”¹ <strong>OrquestraÃ§Ã£o:</strong> Apache Airflow (Astro CLI)</p>
    <p>ğŸ”¹ <strong>TransformaÃ§Ã£o:</strong> dbt (Data Build Tool)</p>
    <p>ğŸ”¹ <strong>Data Platform:</strong> Databricks</p>
    <p>ğŸ”¹ <strong>Qualidade:</strong> Testes automatizados</p>
    
    <p>ğŸš€ <strong>Powered by:</strong> Astro CLI + Apache Airflow + dbt + Databricks</p>
    <p>ğŸ‘¨â€ğŸ’» <strong>Desenvolvido por:</strong> Diego Brito</p>
    ''',
    dag=dag,
)

# Definir dependÃªncias com Task Groups - Fluxo empresarial
setup_group >> dbt_setup_group >> staging_group >> intermediate_group >> marts_group >> qa_group >> finalizar_pipeline >> notificacao_sucesso

# DependÃªncias internas nos grupos
iniciar_pipeline >> verificar_ambiente
limpar_ambiente_dbt >> instalar_dependencias_dbt